{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx3U35iEAKi2",
        "outputId": "95f50eda-f14a-417f-a251-06994b6cad96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mask-CycleGAN'...\n",
            "remote: Enumerating objects: 5416, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 5416 (delta 56), reused 3 (delta 1), pack-reused 5316\u001b[K\n",
            "Receiving objects: 100% (5416/5416), 227.25 MiB | 35.47 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "Updating files: 100% (5411/5411), done.\n",
            "/content/Mask-CycleGAN\n",
            "Mounted at /content/drive\n",
            "Loading model: \n",
            "Loading model: \n",
            "Loading model: \n",
            "Loading model: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 1334/1334 [05:51<00:00,  3.80it/s, A_fake=0.000336, A_real=0.000429, B_fake=0.000253, B_real=0.000587]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for epoch : 0\n",
            "Saving model for epoch : 0\n",
            "Saving model for epoch : 0\n",
            "Saving model for epoch : 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [05:43<00:00,  3.88it/s, A_fake=0.000336, A_real=0.000491, B_fake=0.000257, B_real=0.000481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for epoch : 1\n",
            "Saving model for epoch : 1\n",
            "Saving model for epoch : 1\n",
            "Saving model for epoch : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [05:44<00:00,  3.88it/s, A_fake=0.000268, A_real=0.000545, B_fake=0.000354, B_real=0.000539]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for epoch : 2\n",
            "Saving model for epoch : 2\n",
            "Saving model for epoch : 2\n",
            "Saving model for epoch : 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [05:44<00:00,  3.87it/s, A_fake=0.000292, A_real=0.00046, B_fake=0.000303, B_real=0.000427]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for epoch : 3\n",
            "Saving model for epoch : 3\n",
            "Saving model for epoch : 3\n",
            "Saving model for epoch : 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [05:44<00:00,  3.88it/s, A_fake=0.000272, A_real=0.000629, B_fake=0.000313, B_real=0.000318]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for epoch : 4\n",
            "Saving model for epoch : 4\n",
            "Saving model for epoch : 4\n",
            "Saving model for epoch : 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [05:44<00:00,  3.88it/s, A_fake=0.000271, A_real=0.000333, B_fake=0.000443, B_real=0.000267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for epoch : 5\n",
            "Saving model for epoch : 5\n",
            "Saving model for epoch : 5\n",
            "Saving model for epoch : 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [05:44<00:00,  3.87it/s, A_fake=0.000297, A_real=0.000467, B_fake=0.000325, B_real=0.000448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for epoch : 6\n",
            "Saving model for epoch : 6\n",
            "Saving model for epoch : 6\n",
            "Saving model for epoch : 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [05:44<00:00,  3.87it/s, A_fake=0.000303, A_real=0.000415, B_fake=0.000419, B_real=0.000404]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for epoch : 7\n",
            "Saving model for epoch : 7\n",
            "Saving model for epoch : 7\n",
            "Saving model for epoch : 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 384/1334 [01:42<04:12,  3.76it/s, A_fake=0.00103, A_real=0.00187, B_fake=0.000937, B_real=0.00157]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ad7caca91663>\u001b[0m in \u001b[0;36m<cell line: 329>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ad7caca91663>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;31m#train_fn(disc_W, disc_S, gen_S, gen_W, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler,config.LAMBDA_IDENTITY, config.LAMBDA_CYCLE-epoch*0.15,GAMMA_CYCLE=GAMMA_CYCLE+0.015)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLAMBDA_IDENTITY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLAMBDA_CYCLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ad7caca91663>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(disc_A, disc_B, gen_B, gen_A, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler, LAMBDA_IDENTITY, LAMBDA_CYCLE, GAMMA_CYCLE)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mopt_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Federico6419/Mask-CycleGAN          #It clones our github repository\n",
        "%cd Mask-CycleGAN\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#IMPORTS\n",
        "from dataset import Dataset\n",
        "import config\n",
        "from discriminator import Discriminator\n",
        "from generator import Generator\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "from torch import Tensor\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "######### FUNCTIONS FOR SAVE AND LOAD MODELS #########\n",
        "#This function saves the weights of the model in a file\n",
        "def save_model(model, optimizer, epoch, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"Saving model for epoch : \"+ str(epoch))\n",
        "\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }, filename)\n",
        "\n",
        "\n",
        "#This function loads the precomputed weights of the model from a file\n",
        "def load_model(file, model, optimizer, lr):\n",
        "    print(\"Loading model: \")\n",
        "    model_check = torch.load(file, map_location=config.DEVICE)\n",
        "    model.load_state_dict(model_check[\"state_dict\"])\n",
        "    optimizer.load_state_dict(model_check[\"optimizer\"])\n",
        "\n",
        "    #epoch =model_check[\"epoch\"]\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "######### END FUNCTIONS FOR SAVE AND LOAD MODELS ##########\n",
        "\n",
        "\n",
        "\n",
        "########################### TRAIN FUNCTION #########################\n",
        "def train_fn(disc_A, disc_B, gen_B, gen_A, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler,LAMBDA_IDENTITY, LAMBDA_CYCLE,GAMMA_CYCLE):\n",
        "\n",
        "    loop = tqdm(loader, leave=True)           #leave=True to avoid print newline\n",
        "\n",
        "    for idx, (domainB, domainA) in enumerate(loop):                             #It loops over the images from domain A and domain B\n",
        "        domainA = domainA.to(config.DEVICE)                                     #Its puts the images from the two domains one the device\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "\n",
        "        #Label printed every epoch to see the prediction of the discriminators\n",
        "        A_is_real = 0\n",
        "        A_is_fake = 0\n",
        "        B_is_real = 0\n",
        "        B_is_fake = 0\n",
        "\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            ############## TRAIN DISCRIMINATOR DOMAIN B #############\n",
        "            fake_B = gen_B(domainA)              #Generate with Generator a fake image from domain B starting from an image from domain A\n",
        "\n",
        "            #Compute probability of the real image and of the fake image to be a real image from domain B using the Discriminator\n",
        "            D_B_real = disc_B(domainB)\n",
        "            D_B_fake = disc_B(fake_B.detach())\n",
        "\n",
        "            #Used to print the percentage that the given image is predicted real or fake !!!!\n",
        "            B_is_real += D_B_real.mean().item()\n",
        "            B_is_fake += D_B_fake.mean().item()\n",
        "\n",
        "            #Compute the Mean Squared Error\n",
        "            D_B_real_loss = mse(D_B_real, torch.ones_like(D_B_real))    #MSE computed between the prediction of the real image made by Discriminator and a Tensor composed by all ones\n",
        "            D_B_fake_loss = mse(D_B_fake, torch.zeros_like(D_B_fake))   #MSE computed between the prediction of the fake image made by Discriminator and a Tensor composed by all zeros\n",
        "            D_B_loss = D_B_real_loss + D_B_fake_loss                    #Sum the real image loss and the fake image loss\n",
        "\n",
        "\n",
        "\n",
        "            ########### TRAIN DISCRIMINATOR OF THE DOMAIN A ##############\n",
        "            fake_A = gen_A(domainB)             #Generate with Generator a fake image from domain A starting from an image from domain B\n",
        "\n",
        "            #Compute probability of the real image and of the fake image to be a real image from domain B using the Discriminator\n",
        "            D_A_real = disc_A(domainA)\n",
        "            D_A_fake = disc_A(fake_A.detach())\n",
        "\n",
        "            #Used print the percentage that the given image is predicted real or fake !!!!\n",
        "            A_is_real += D_A_real.mean().item()\n",
        "            A_is_fake += D_A_fake.mean().item()\n",
        "\n",
        "            #Compute the Mean Squared Error\n",
        "            D_A_real_loss = mse(D_A_real, torch.ones_like(D_A_real))    #MSE computed between the prediction of the real image made by Discriminator and a Tensor composed by all ones\n",
        "            D_A_fake_loss = mse(D_A_fake, torch.zeros_like(D_A_fake))   #MSE computed between the prediction of the fake image made by Discriminator and a Tensor composed by all zeros\n",
        "            D_A_loss = D_A_real_loss + D_A_fake_loss                    #Sum the real image loss and the fake image loss\n",
        "\n",
        "\n",
        "\n",
        "            #Put together the loss of the two discriminators\n",
        "            D_loss = (D_A_loss + D_B_loss)/2\n",
        "\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        d_scaler.scale(D_loss).backward(retain_graph=True)\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "\n",
        "\n",
        "        ########################## TRAIN GENERATORS #########################\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            #Compute the Discriminator predictions\n",
        "            D_A_fake = disc_A(fake_A)\n",
        "            D_B_fake = disc_B(fake_B)\n",
        "            D_A_real = disc_A(domainA)\n",
        "            D_B_real = disc_B(domainB)\n",
        "\n",
        "            #Compute the GAN losses\n",
        "            loss_G_A = 0\n",
        "            loss_G_B = 0\n",
        "\n",
        "            loss_G_A = mse(D_A_fake, torch.ones_like(D_A_fake))  #Compute the MSE between the prediction of the Discriminator of the fake image from domain A and a tensor with all ones\n",
        "            loss_G_B = mse(D_B_fake, torch.ones_like(D_B_fake))  #Compute the MSE between the prediction of the Discriminator of the fake image from domain B and a tensor with all ones\n",
        "\n",
        "            \"\"\"\n",
        "            #BETTER CYCLE CONSISTENCY\n",
        "            if(config.BETTER):\n",
        "                ################ BETTER CYCLE CONSISTENCY FOLLOWING THE REPORT TIPS #################\n",
        "                cycle_summer = gen_S(fake_winter)\n",
        "                x = disc_S(summer,feature_extract = True)\n",
        "                Fx = disc_S(cycle_summer,feature_extract = True)\n",
        "                norma_summer=l1(x,Fx)\n",
        "                cycle_summer_loss = l1(summer, cycle_summer)\n",
        "\n",
        "                cycle_winter = gen_W(fake_summer)\n",
        "                y = disc_W(winter,feature_extract = True)\n",
        "                Fy = disc_W(cycle_winter,feature_extract = True)\n",
        "                norma_winter=l1(y,Fy)\n",
        "                cycle_winter_loss = l1(winter, cycle_winter)\n",
        "\n",
        "                G_loss = (\n",
        "                loss_G_S\n",
        "                + loss_G_W\n",
        "                + torch.mean(disc_W(winter))*(GAMMA_CYCLE * norma_winter + (1-GAMMA_CYCLE) * cycle_winter_loss) * LAMBDA_CYCLE\n",
        "                + torch.mean(disc_S(summer))*(GAMMA_CYCLE * norma_summer+ (1-GAMMA_CYCLE) * cycle_summer_loss) * LAMBDA_CYCLE\n",
        "                )\n",
        "                ################ BETTER CYCLE CONSISTENCY FOLLOWING THE REPORT TIPS #################\n",
        "            else:\n",
        "              \"\"\"\n",
        "\n",
        "            #CYCLE LOSS\n",
        "            cycle_A = gen_A(fake_B)\n",
        "            cycle_B = gen_B(fake_A)\n",
        "            cycle_A_loss = l1(domainA, cycle_A)\n",
        "            cycle_B_loss = l1(domainB, cycle_B)\n",
        "\n",
        "            #IDENTITY LOSS\n",
        "            identity_A = gen_A(domainA)\n",
        "            identity_B = gen_B(domainB)\n",
        "            identity_loss_A = l1(domainA, identity_A)\n",
        "            identity_loss_B = l1(domainB, identity_B)\n",
        "\n",
        "            #Add all losses together, multiplied by their relative parameter\n",
        "            G_loss = (\n",
        "                loss_G_B\n",
        "                + loss_G_A\n",
        "                + cycle_B_loss * LAMBDA_CYCLE\n",
        "                + cycle_A_loss * LAMBDA_CYCLE\n",
        "                + identity_loss_A * LAMBDA_IDENTITY\n",
        "                + identity_loss_B * LAMBDA_IDENTITY\n",
        "            )\n",
        "\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward(retain_graph=True)\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        ##########################  END TRAIN GENERATORS #########################\n",
        "\n",
        "\n",
        "        #Save tensors into images every 150 to see in real time the progress of the net\n",
        "        if idx % 150 == 0:\n",
        "            save_image(fake_B*0.5+0.5, f\"Saved_Images/domainB_{idx}.png\")\n",
        "            save_image(fake_A*0.5+0.5, f\"Saved_Images/domainA_{idx}.png\")\n",
        "\n",
        "        #Set postfixes to the progess bar of tqdm\n",
        "        #loop.set_postfix(A_real=A_is_real/(idx+1), A_fake=A_is_fake/(idx+1),B_real=B_is_real/(idx+1), B_fake=B_is_fake/(idx+1))\n",
        "        loop.set_postfix(G_loss=G_loss.item(), D_loss=D_loss.item(), cycle_A_loss=cycle_A_loss, cycle_B_loss=cycle_B_loss)\n",
        "\n",
        "########################### END TRAIN FUNCTION ######################\n",
        "\n",
        "\n",
        "\n",
        "#TEST FUNCTIONS\n",
        "#Test function for Domain A\n",
        "def test_fn_A(gen_B, gen_A, test_loader):\n",
        "\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "\n",
        "    for idx, (domainB, domainA) in enumerate(loop):\n",
        "        domainA = domainA.to(config.DEVICE)\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "        fake_B = gen_B(domainA)\n",
        "        fake_A = gen_A(fake_B)\n",
        "\n",
        "        save_image(domainA*0.5+0.5, f\"test_images/testoriginal_{idx}.png\")\n",
        "        save_image(fake_B*0.5+0.5, f\"test_images/testdomainB_{idx}.png\")\n",
        "        save_image(fake_A*0.5+0.5, f\"test_images/testdomainA_{idx}.png\")\n",
        "\n",
        "#Test function for Domain B\n",
        "def test_fn_B(gen_B,gen_A,test_loader):\n",
        "\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "\n",
        "    for idx, (domainB, domainA) in enumerate(loop):\n",
        "        domainA = domainA.to(config.DEVICE)\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "        fake_A = gen_A(domainB)\n",
        "        fake_B = gen_B(fake_A)\n",
        "\n",
        "        save_image(domainB*0.5+0.5, f\"test_images/testoriginal_{idx}.png\")\n",
        "        save_image(fake_B*0.5+0.5, f\"test_images/testdomainB_{idx}.png\")\n",
        "        save_image(fake_A*0.5+0.5, f\"test_images/testdomainA_{idx}.png\")\n",
        "\n",
        "\n",
        "###################### MAIN FUNCTION #######################\n",
        "def main():\n",
        "    #Initialize Discriminators and Generators\n",
        "    disc_A = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    disc_B = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    gen_A = Generator(img_channels=3).to(config.DEVICE)\n",
        "    gen_B = Generator(img_channels=3).to(config.DEVICE)\n",
        "\n",
        "    #Adam for Discriminators\n",
        "    opt_disc = optim.Adam(\n",
        "        list(disc_A.parameters()) + list(disc_B.parameters()),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    #Adam for Generators\n",
        "    opt_gen = optim.Adam(\n",
        "        list(gen_B.parameters()) + list(gen_A.parameters()),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    #Define L1 and Mean Squared Error loss\n",
        "    L1 = nn.L1Loss()\n",
        "    mse = nn.MSELoss()\n",
        "\n",
        "    #GAMMA_CYCLE = config.GAMMA_CYCLE # ratio between discriminator CNN feature level and pixel level loss   !!!!!\n",
        "\n",
        "    #Load pretrained model\n",
        "    if config.LOAD_MODEL:\n",
        "        load_model(\n",
        "            config.CHECKPOINT_GEN_A, gen_A, opt_gen, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_GEN_B, gen_B, opt_gen, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_DISC_A, disc_A, opt_disc, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_DISC_B, disc_B, opt_disc, config.LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "\n",
        "    ############## CHOICE OF THE DATASET ###############\n",
        "    if(config.TRANSFORMATION == \"WinterToSummer\"):\n",
        "        dataset = Dataset(\n",
        "            domainA_dir=config.TRAIN_DIR+\"/trainWinter\", domainB_dir=config.TRAIN_DIR+\"/trainSummer\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            domainA_dir=config.TEST_DIR+\"/testWinter\", domainB_dir=config.TEST_DIR+\"/testSummer\", transform=config.transforms\n",
        "        )\n",
        "    elif(config.TRANSFORMATION == \"HorseToZebra\"):\n",
        "        dataset = Dataset(\n",
        "            domainA_dir=config.TRAIN_DIR+\"/trainHorse\", domainB_dir=config.TRAIN_DIR+\"/trainZebra\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            domainA_dir=config.TEST_DIR+\"/testHorse\", domainB_dir=config.TEST_DIR+\"/testZebra\", transform=config.transforms\n",
        "        )\n",
        "    elif(config.TRANSFORMATION == \"MonetToPhoto\"):\n",
        "        dataset = Dataset(\n",
        "            domainA_dir=config.TRAIN_DIR+\"/trainMonet\", domainB_dir=config.TRAIN_DIR+\"/trainPhotoMonet\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            domainA_dir=config.TEST_DIR+\"/testMonet\", domainB_dir=config.TEST_DIR+\"/testPhotoMonet\", transform=config.transforms\n",
        "        )\n",
        "\n",
        "\n",
        "    ############# DATALOADER #############\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True  #for faster training(non-paged cpu memory)\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    #Define the scalers\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    #Train the model\n",
        "    if(config.TRAIN_MODEL):\n",
        "\n",
        "        for epoch in range(config.NUM_EPOCHS):\n",
        "            #if(config.BETTER): !!!!\n",
        "                #train_fn(disc_W, disc_S, gen_S, gen_W, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler,config.LAMBDA_IDENTITY, config.LAMBDA_CYCLE-epoch*0.15,GAMMA_CYCLE=GAMMA_CYCLE+0.015)\n",
        "            #else:\n",
        "\n",
        "            #Set the models in training mode\n",
        "            disc_A.train()\n",
        "            disc_B.train()\n",
        "            gen_A.train()\n",
        "            gen_B.train()\n",
        "            train_fn(disc_A, disc_B, gen_B, gen_A, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler,config.LAMBDA_IDENTITY, config.LAMBDA_CYCLE, 0)\n",
        "\n",
        "            #If SAVE_MODEL is set to True save the current model\n",
        "            if config.SAVE_MODEL:\n",
        "                save_model(gen_A, opt_gen, epoch ,filename=config.NEW_CHECKPOINT_GEN_A)\n",
        "                save_model(gen_B, opt_gen, epoch , filename=config.NEW_CHECKPOINT_GEN_B)\n",
        "                save_model(disc_A, opt_disc, epoch , filename=config.NEW_CHECKPOINT_DISC_A)\n",
        "                save_model(disc_B, opt_disc, epoch , filename=config.NEW_CHECKPOINT_DISC_B)\n",
        "\n",
        "    #Test the model\n",
        "    else:\n",
        "        #Set the models in evaluation mode\n",
        "        disc_A.eval()\n",
        "        disc_B.eval()\n",
        "        gen_A.eval()\n",
        "        gen_B.eval()\n",
        "\n",
        "        test_fn_A(gen_B,gen_A,test_loader)        #Test on Domain A\n",
        "        #test_fn_B(gen_B,gen_A,test_loader)       #Test on Domain B\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}