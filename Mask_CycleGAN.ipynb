{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx3U35iEAKi2",
        "outputId": "d16bdebd-9ea4-4df4-fd24-5f25c8d77d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask-CycleGAN'...\n",
            "remote: Enumerating objects: 2671, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 2671 (delta 50), reused 52 (delta 20), pack-reused 2567\u001b[K\n",
            "Receiving objects: 100% (2671/2671), 117.69 MiB | 22.55 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "/content/Mask-CycleGAN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 41%|████      | 499/1231 [02:09<03:10,  3.85it/s, A_fake=0.000929, A_real=0.00119, B_fake=0.000876, B_real=0.00117]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6a80d92ac431>\u001b[0m in \u001b[0;36m<cell line: 352>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6a80d92ac431>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;31m#train_fn(disc_W, disc_S, gen_S, gen_W, loader, opt_disc, opt_gen, L1, mse, BCE, d_scaler, g_scaler,config.LAMBDA_IDENTITY, config.LAMBDA_CYCLE-epoch*0.15,GAMMA_CYCLE=GAMMA_CYCLE+0.015)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLAMBDA_IDENTITY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLAMBDA_CYCLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6a80d92ac431>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(disc_A, disc_B, gen_B, gen_A, loader, opt_disc, opt_gen, l1, mse, BCE, d_scaler, g_scaler, LAMBDA_IDENTITY, LAMBDA_CYCLE, GAMMA_CYCLE)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopt_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mg_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Federico6419/Mask-CycleGAN\n",
        "%cd Mask-CycleGAN\n",
        "\n",
        "from dataset import Dataset\n",
        "import config\n",
        "from discriminator import Discriminator\n",
        "from generator import Generator\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "from torch import Tensor\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "######### FUNCTION FOR SAVE AND LOAD MODELS #########\n",
        "\n",
        "def save_model(model, optimizer, epoch, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"Saving model for epoch : \"+ str(epoch))\n",
        "\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }, filename)\n",
        "\n",
        "\n",
        "def load_model(file, model, optimizer, lr):\n",
        "    print(\"Loading model: \")\n",
        "    model_check = torch.load(file, map_location=config.DEVICE)\n",
        "    model.load_state_dict(model_check[\"state_dict\"])\n",
        "    optimizer.load_state_dict(model_check[\"optimizer\"])\n",
        "\n",
        "    #epoch =model_check[\"epoch\"]\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "######### END FUNCTION FOR MODELS ##########\n",
        "\n",
        "def gradient_penalty(model, real_images, fake_images, device):\n",
        "    # Random weight term for interpolation between real and fake data\n",
        "    alpha = torch.randn((real_images.size(0), 1, 1, 1), device=device)\n",
        "    print(alpha)\n",
        "    # Get random interpolation between real and fake data\n",
        "    interpolates = (alpha * real_images + ((1 - alpha) * fake_images)).requires_grad_(True)\n",
        "\n",
        "    model_interpolates = model(interpolates)\n",
        "    grad_outputs = torch.ones(model_interpolates.size(), device=device, requires_grad=False)\n",
        "\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=model_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=grad_outputs,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = torch.mean((gradients.norm(2, dim=1) - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "\n",
        "                ########################### TRAIN FUNCTION #########################\n",
        "def train_fn(disc_A, disc_B, gen_B, gen_A, loader, opt_disc, opt_gen, l1, mse, BCE, d_scaler, g_scaler,LAMBDA_IDENTITY, LAMBDA_CYCLE,GAMMA_CYCLE):\n",
        "\n",
        "    loop = tqdm(loader, leave=True)           #leave=True to avoid print newline\n",
        "\n",
        "    # Loss weight for gradient penalty\n",
        "    LAMBDA_GP = 10\n",
        "\n",
        "    for idx, (domainB, domainA) in enumerate(loop):\n",
        "        domainA = domainA.to(config.DEVICE)\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "        # Ground truths used in the adversarial loss\n",
        "        \"\"\"\n",
        "        validA = Variable(Tensor(domainA.shape[0], 1,30,30).fill_(1.0), requires_grad=False)\n",
        "        validB = Variable(Tensor(domainB.shape[0], 1,30,30).fill_(1.0), requires_grad=False)\n",
        "\n",
        "        fakeA = Variable(Tensor(domainA.shape[0], 1,30,30).fill_(0.0), requires_grad=False)\n",
        "        fakeB = Variable(Tensor(domainB.shape[0], 1,30,30).fill_(0.0), requires_grad=False)\n",
        "        \"\"\"\n",
        "\n",
        "        # Label printed every epoch to see the prediction of the discriminators\n",
        "        A_is_real = 0\n",
        "        A_is_fake = 0\n",
        "        B_is_real = 0\n",
        "        B_is_fake = 0\n",
        "\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            \"\"\"\n",
        "            validA = validA.to(config.DEVICE)\n",
        "            fakeA = fakeA.to(config.DEVICE)\n",
        "            validB = validB.to(config.DEVICE)\n",
        "            fakeB = fakeB.to(config.DEVICE)\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "            ############## TRAIN DISCRIMINATOR DOMAIN B #############\n",
        "            fake_B = gen_B(domainA)              #Generate a fake image from domain B starting from an image from domain A\n",
        "\n",
        "            #Compute probability to be a real image from domain B using the discriminator\n",
        "            D_B_real = disc_B(domainB)\n",
        "            D_B_fake = disc_B(fake_B.detach())\n",
        "\n",
        "            #Used to print the percentage that the given image is predicted real or fake !!!!!!!!!!!\n",
        "            B_is_real += D_B_real.mean().item()\n",
        "            B_is_fake += D_B_fake.mean().item()\n",
        "\n",
        "\n",
        "            D_B_real_loss = mse(D_B_real, torch.ones_like(D_B_real))\n",
        "            D_B_fake_loss = mse(D_B_fake, torch.zeros_like(D_B_fake))\n",
        "            D_B_loss = D_B_real_loss + D_B_fake_loss\n",
        "\n",
        "\n",
        "\n",
        "            ########### TRAIN DISCRIMINATOR OF THE DOMAIN B ##############\n",
        "            fake_A = gen_A(domainB)\n",
        "            D_A_real = disc_A(domainA)\n",
        "            D_A_fake = disc_A(fake_A.detach())\n",
        "            #used print the percentage that the given image is predicted real or fake\n",
        "            A_is_real += D_A_real.mean().item()\n",
        "            A_is_fake += D_A_fake.mean().item()\n",
        "\n",
        "            D_A_real_loss = mse(D_A_real, torch.ones_like(D_A_real))\n",
        "            D_A_fake_loss = mse(D_A_fake, torch.zeros_like(D_A_fake))\n",
        "            D_A_loss = D_A_real_loss + D_A_fake_loss\n",
        "\n",
        "\n",
        "\n",
        "            # put togheter the loss of the two discriminators\n",
        "            D_loss = (D_A_loss + D_B_loss)/2\n",
        "\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        d_scaler.scale(D_loss).backward(retain_graph=True)\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "\n",
        "\n",
        "        ########################## TRAIN GENERATORS #########################\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            D_A_fake = disc_A(fake_A)\n",
        "            D_B_fake = disc_B(fake_B)\n",
        "            D_A_real = disc_A(domainA)\n",
        "            D_B_real = disc_B(domainB)\n",
        "\n",
        "            loss_G_A = 0\n",
        "            loss_G_B = 0\n",
        "\n",
        "            loss_G_A = mse(D_A_fake, torch.ones_like(D_A_fake))\n",
        "            loss_G_B = mse(D_B_fake, torch.ones_like(D_B_fake))\n",
        "\n",
        "            \"\"\"\n",
        "            if(config.BETTER):\n",
        "                ################ BETTER CYCLE CONSISTENCY FOLLOWING THE REPORT TIPS #################\n",
        "                cycle_summer = gen_S(fake_winter)\n",
        "                x = disc_S(summer,feature_extract = True)\n",
        "                Fx = disc_S(cycle_summer,feature_extract = True)\n",
        "                norma_summer=l1(x,Fx)\n",
        "                cycle_summer_loss = l1(summer, cycle_summer)\n",
        "\n",
        "                cycle_winter = gen_W(fake_summer)\n",
        "                y = disc_W(winter,feature_extract = True)\n",
        "                Fy = disc_W(cycle_winter,feature_extract = True)\n",
        "                norma_winter=l1(y,Fy)\n",
        "                cycle_winter_loss = l1(winter, cycle_winter)\n",
        "\n",
        "                G_loss = (\n",
        "                loss_G_S\n",
        "                + loss_G_W\n",
        "                + torch.mean(disc_W(winter))*(GAMMA_CYCLE * norma_winter + (1-GAMMA_CYCLE) * cycle_winter_loss) * LAMBDA_CYCLE\n",
        "                + torch.mean(disc_S(summer))*(GAMMA_CYCLE * norma_summer+ (1-GAMMA_CYCLE) * cycle_summer_loss) * LAMBDA_CYCLE\n",
        "                )\n",
        "                ################ BETTER CYCLE CONSISTENCY FOLLOWING THE REPORT TIPS #################\n",
        "            else:\n",
        "              \"\"\"\n",
        "\n",
        "            #CYCLE LOSS\n",
        "            cycle_A = gen_A(fake_A)\n",
        "            cycle_B = gen_B(fake_B)\n",
        "            cycle_A_loss = l1(domainA, cycle_A)\n",
        "            cycle_B_loss = l1(domainB, cycle_B)\n",
        "\n",
        "            #IDENTITY LOSS\n",
        "            identity_A = gen_A(domainA)\n",
        "            identity_B = gen_B(domainB)\n",
        "            identity_loss_A = l1(domainA, identity_A)\n",
        "            identity_loss_B = l1(domainB, identity_B)\n",
        "\n",
        "            #Add all losses together\n",
        "            G_loss = (\n",
        "                loss_G_B\n",
        "                + loss_G_A\n",
        "                + cycle_B_loss * LAMBDA_CYCLE\n",
        "                + cycle_A_loss * LAMBDA_CYCLE\n",
        "                + identity_loss_A * LAMBDA_IDENTITY\n",
        "                + identity_loss_B * LAMBDA_IDENTITY\n",
        "            )\n",
        "\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward(retain_graph=True)\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        ##########################  END TRAIN GENERATORS #########################\n",
        "\n",
        "\n",
        "        if idx % 150 == 0:    #save tensor into images every 150 to see in real time the progress of the net\n",
        "            save_image(fake_B*0.5+0.5, f\"Saved_Images/domainB_{idx}.png\")\n",
        "            save_image(fake_A*0.5+0.5, f\"Saved_Images/domainA_{idx}.png\")\n",
        "\n",
        "        #set postfixes to the progess bar of tqdm\n",
        "        loop.set_postfix(A_real=A_is_real/(idx+1), A_fake=A_is_fake/(idx+1),B_real=B_is_real/(idx+1), B_fake=B_is_fake/(idx+1))\n",
        "\n",
        "                ########################### END TRAIN FUNCTION ######################\n",
        "\n",
        "\n",
        "\n",
        "#TEST FUNCTIONS\n",
        "def test_fn_A(gen_B,gen_A,test_loader):\n",
        "\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "\n",
        "    for idx, (domainB, domainA) in enumerate(loop):\n",
        "        domainA = domainA.to(config.DEVICE)\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "        fake_B = gen_B(domainA)\n",
        "        fake_A = gen_A(fake_B)\n",
        "\n",
        "        save_image(domainA*0.5+0.5, f\"test_images/testoriginal_{idx}.png\")\n",
        "        save_image(fake_B*0.5+0.5, f\"test_images/testdomainB_{idx}.png\")\n",
        "        save_image(fake_A*0.5+0.5, f\"test_images/testdomainA_{idx}.png\")\n",
        "\n",
        "def test_fn_B(gen_B,gen_A,test_loader):\n",
        "\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "\n",
        "    for idx, (domainB, domainA) in enumerate(loop):\n",
        "        domainA = domainA.to(config.DEVICE)\n",
        "        domainB = domainB.to(config.DEVICE)\n",
        "        fake_A = gen_A(domainB)\n",
        "        fake_B = gen_B(fake_A)\n",
        "\n",
        "        save_image(domainB*0.5+0.5, f\"test_images/testoriginal_{idx}.png\")\n",
        "        save_image(fake_B*0.5+0.5, f\"test_images/testdomainB_{idx}.png\")\n",
        "        save_image(fake_A*0.5+0.5, f\"test_images/testdomainA_{idx}.png\")\n",
        "\n",
        "                        ###################### MAIN FUNCTION #######################\n",
        "def main():\n",
        "    disc_A = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    disc_B = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    gen_A = Generator(img_channels=3).to(config.DEVICE)\n",
        "    gen_B = Generator(img_channels=3).to(config.DEVICE)\n",
        "\n",
        "    opt_disc = optim.Adam(\n",
        "        list(disc_A.parameters()) + list(disc_B.parameters()),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    opt_gen = optim.Adam(\n",
        "        list(gen_B.parameters()) + list(gen_A.parameters()),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    L1 = nn.L1Loss()\n",
        "    mse = nn.MSELoss()\n",
        "    BCE = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    #GAMMA_CYCLE = config.GAMMA_CYCLE # ratio between discriminator CNN feature level and pixel level loss   !!!!!\n",
        "\n",
        "    if config.LOAD_MODEL:\n",
        "        \"\"\"load_model(      !!!!\n",
        "            config.CHECKPOINT_GEN_A, gen_A, opt_gen, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_GEN_B, gen_B, opt_gen, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_DISC_A, disc_A, opt_disc, config.LEARNING_RATE,\n",
        "        )\n",
        "        load_model(\n",
        "            config.CHECKPOINT_DISC_B, disc_B, opt_disc, config.LEARNING_RATE,\n",
        "        )\"\"\"\n",
        "\n",
        "\n",
        "    ############## CHOICE OF THE DATASET ###############  !!!!\n",
        "    if(config.TRANSFORMATION == \"WinterToSummer\"):\n",
        "        dataset = Dataset(\n",
        "            winter_dir=config.TRAIN_DIR+\"/trainWinter\", summer_dir=config.TRAIN_DIR+\"/trainSummer\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            winter_dir=config.TEST_DIR+\"/testWinter\", summer_dir=config.TEST_DIR+\"/testSummer\", transform=config.transforms\n",
        "        )\n",
        "    elif(config.TRANSFORMATION == \"HorseToZebra\"):\n",
        "        dataset = Dataset(\n",
        "            horse_dir=config.TRAIN_DIR+\"/trainHorse\", zebra_dir=config.TRAIN_DIR+\"/trainZebra\", transform=config.transforms\n",
        "        )\n",
        "        test_dataset = Dataset(\n",
        "            horse_dir=config.TEST_DIR+\"/testHorse\", zebra_dir=config.TEST_DIR+\"/testZebra\", transform=config.transforms\n",
        "        )\n",
        "\n",
        "     ############# CHOICE OF THE DATASET ##############\n",
        "\n",
        "    ############# DATALOADER #############\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True  #for faster training(non-paged cpu memory)\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    ############# DATALOADER ###############\n",
        "\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    if(config.TRAIN_MODEL):\n",
        "\n",
        "        for epoch in range(config.NUM_EPOCHS):\n",
        "            #if(config.BETTER): !!!!\n",
        "                #train_fn(disc_W, disc_S, gen_S, gen_W, loader, opt_disc, opt_gen, L1, mse, BCE, d_scaler, g_scaler,config.LAMBDA_IDENTITY, config.LAMBDA_CYCLE-epoch*0.15,GAMMA_CYCLE=GAMMA_CYCLE+0.015)\n",
        "            #else:\n",
        "            train_fn(disc_A, disc_B, gen_B, gen_A, loader, opt_disc, opt_gen, L1, mse, BCE, d_scaler, g_scaler,config.LAMBDA_IDENTITY, config.LAMBDA_CYCLE, 0)\n",
        "\n",
        "\n",
        "            if config.SAVE_MODEL: #if save_Model is set to true save model on the specific path\n",
        "                save_model(gen_A, opt_gen, epoch ,filename=config.CHECKPOINT_GEN_A)\n",
        "                save_model(gen_B, opt_gen, epoch , filename=config.CHECKPOINT_GEN_B)\n",
        "                save_model(disc_A, opt_disc, epoch , filename=config.CHECKPOINT_DISC_A)\n",
        "                save_model(disc_B, opt_disc, epoch , filename=config.CHECKPOINT_DISC_B)\n",
        "    else:\n",
        "\n",
        "        test_fn_A(gen_B,gen_A,test_loader)\n",
        "        #test_fn_B(gen_B,gen_A,test_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}
